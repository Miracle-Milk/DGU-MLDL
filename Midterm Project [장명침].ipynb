{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65b673d",
   "metadata": {},
   "source": [
    "# Kaggle Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01a265",
   "metadata": {},
   "source": [
    "## Describe Your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2c779",
   "metadata": {},
   "source": [
    "**URL:** https://www.kaggle.com/datasets/gpiosenka/cards-image-datasetclassification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee95bd2",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Identify the suits and points of playing cards, divided into 53 categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b65df",
   "metadata": {},
   "source": [
    "**Datasets**\n",
    "\n",
    "* Train dataset: 7624 card Image, 224*224 3 channel\n",
    "\n",
    "* Validation dataset: 265 Image, 224*224 3 channel\n",
    "\n",
    "* Test dataset: 265 Image, 224*224 3 channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd627a2",
   "metadata": {},
   "source": [
    "**Features(x):**\n",
    "\n",
    "suits: clubs, diamonds, hearts, spades  \n",
    "points: ace, two, three, four, five, six, seven, eight, nine, ten, jack, queen, king, jocker\n",
    "| suits   | \n",
    "| ------ | \n",
    "| ♠️     | \n",
    "| ♥️     | \n",
    "| ♦️     | \n",
    "| ♣️     |   \n",
    "\n",
    "---\n",
    "\n",
    "| points    |\n",
    "| ------- |\n",
    "| A, 2-10, J, Q, K |\n",
    "| A, 2-10, J, Q, K |\n",
    "| A, 2-10, J, Q, K |\n",
    "| A, 2-10, J, Q, K |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cad80b",
   "metadata": {},
   "source": [
    "**Target(y):**  \n",
    "\n",
    "playing card classification\n",
    "\n",
    "suits & points:  \n",
    "\n",
    "| suits   | points    |\n",
    "| ------ | ------- |\n",
    "| ♠️     | A, 2-10, J, Q, K |\n",
    "| ♥️     | A, 2-10, J, Q, K |\n",
    "| ♦️     | A, 2-10, J, Q, K |\n",
    "| ♣️     | A, 2-10, J, Q, K |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df73f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccb4a7",
   "metadata": {},
   "source": [
    "## Build Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c8d37",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "486febaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "## codes\n",
    "import os.path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader as D\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU or CPU\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "# dataset path\n",
    "image_path = ''\n",
    "batch_size = 128  # batch size\n",
    "torch.manual_seed(42)  #  set random seed to ensure the same split everytime\n",
    "\n",
    "# data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize image\n",
    "    transforms.CenterCrop(224),  # clip image from center\n",
    "    transforms.RandomHorizontalFlip(),  # random horizontal flip\n",
    "    transforms.RandomRotation(10),  # random rotation\n",
    "    transforms.ToTensor(),  # transform to tensor\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize image\n",
    "    transforms.CenterCrop(224),  # clip image from center\n",
    "    transforms.ToTensor(),  # transform to tensor\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(image_path)\n",
    "\n",
    "# read dataset\n",
    "train_dataset = ImageFolder(image_path + '/train', transform= transform)\n",
    "val_dataset = ImageFolder(image_path + '/valid', transform = transform_test)\n",
    "test_dataset = ImageFolder(image_path + '/test', transform = transform_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = D(train_dataset, batch_size=batch_size, shuffle=True)  # random shuffle\n",
    "val_loader = D(val_dataset, batch_size=batch_size, shuffle=False)  # not shuffle\n",
    "test_loader = D(test_dataset, batch_size=batch_size, shuffle=False)  # not shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5ee14",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c15d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## codes\n",
    "from typing import Union, Type, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels,out_channels,stride=1):\n",
    "    return nn.Conv2d(in_channels,out_channels,3,stride,1,bias=False)\n",
    "\n",
    "def conv1x1(in_channels,out_channels,stride=1):\n",
    "    return nn.Conv2d(in_channels,out_channels,1,stride,0,bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    # basic block\n",
    "    expansion = 1 # channel expansion\n",
    "    def __init__(self,in_channels,out_channels,stride=1,\n",
    "                    downsample=None,\n",
    "                    groups=1,\n",
    "                    base_width=64,\n",
    "                    dilation=1,\n",
    "                    norm_layer=None\n",
    "                 ): \n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels,out_channels,stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) # batch normalization\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels,out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # if the input and output channel is not the same, then downsample\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # if the input and output channel is not the same, then downsample\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # residual connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # bottleneck block\n",
    "\n",
    "    expansion = 4 # channel expansion\n",
    "\n",
    "    def __init__(self, in_channels,out_channels,stride=1,\n",
    "                    downsample=None,\n",
    "                    groups=1,\n",
    "                    base_width=64,\n",
    "                    dilation=1,\n",
    "                    norm_layer=None):\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d # batch normalization\n",
    "\n",
    "        width = int(out_channels * (base_width / 64.)) # calculate the width of each layer\n",
    "\n",
    "        self.conv1 = conv1x1(in_channels,width) # 1x1 convolution\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width,width,stride,dilation) # 3x3 convolution\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width,out_channels * self.expansion ) # 1x1 convolution\n",
    "        self.bn3 = norm_layer(out_channels * self.expansion )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 block:Type[Union[BasicBlock,Bottleneck]],\n",
    "                 layers:List[int],):\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        self._norm_layer = nn.BatchNorm2d\n",
    "        self.in_channels = 64\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.groups = 1\n",
    "        self.base_width = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, 7, 2, 3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
    "\n",
    "        # ResNet-18 has 4 stages with [2, 2, 2, 2] BasicBlocks\n",
    "        self.layer1 = self.make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 128, layers[0], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 256, layers[0], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 512, layers[0], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 7x7 convolution\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x) # 3x3 pooling\n",
    "\n",
    "        x = self.layer1(x) # residual block\n",
    "        x = self.layer2(x) # residual block\n",
    "        x = self.layer3(x) # residual block\n",
    "        x = self.layer4(x) # residual block\n",
    "\n",
    "        x = self.avgpool(x) # average pooling\n",
    "        x = torch.flatten(x,1) # flatten\n",
    "        x = self.fc(x) # fully connected layer\n",
    "\n",
    "        return x\n",
    "\n",
    "    def make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_channels, out_channels * block.expansion, stride),\n",
    "                self._norm_layer(out_channels * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers.append(block(\n",
    "            self.in_channels,\n",
    "            out_channels,\n",
    "            stride,\n",
    "            downsample,\n",
    "            self.groups,\n",
    "            self.base_width,\n",
    "            previous_dilation,\n",
    "            self._norm_layer)\n",
    "        )\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(\n",
    "                self.in_channels,\n",
    "                out_channels,\n",
    "                groups=self.groups,\n",
    "                base_width=self.base_width,\n",
    "                dilation=self.dilation,\n",
    "                norm_layer=self._norm_layer\n",
    "            ))\n",
    "\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814c018",
   "metadata": {},
   "source": [
    "### Train Model & Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dcbadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "## codes\n",
    "def validate(model, val_loader):\n",
    "    # validate the model, return the accuracy\n",
    "    correct = 0 # predict correct number\n",
    "    total = 0   # total number\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "model = ResNet18(53, BasicBlock, [2, 2, 2, 2])\n",
    "model.to(device)\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define optimizer,Adam is a kind of gradient descent algorithm,not sensitive to hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "correct = 0\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0  # every epoch loss\n",
    "\n",
    "    # use tqdm to show the progress bar\n",
    "    progress_bar = tqdm(train_loader,desc='Epoch {:1d}'.format(epoch),leave=False,disable=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        # \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # gradient clear\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()  # update parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(inputs))})\n",
    "\n",
    "    # use validate dataset to validate the model, each 5 epoch\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        acc = validate(model, val_loader)\n",
    "    #   save the best model\n",
    "        if(acc > correct):\n",
    "            correct = acc\n",
    "            torch.save(model.state_dict(), 'model.ckpt')  # save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dea6f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779c8f2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f49b4961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.45283018867924 %\n"
     ]
    }
   ],
   "source": [
    "## codes\n",
    "\n",
    "if(os.path.exists('model.ckpt')):\n",
    "    model.load_state_dict(torch.load('model.ckpt'))\n",
    "model.eval()  # set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# test the model\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        # get the predicted label\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        # calculate the accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2652e",
   "metadata": {},
   "source": [
    "The results explains:\n",
    "\n",
    "Accuracy of the network on the test images: **92.45283018867924 %**\n",
    "\n",
    "- **Data Set**\n",
    "  - We used a dataset containing a large number of playing card images during training and testing of ResNet-18.\n",
    "- **Neural Network**\n",
    "  - ResNet-18 is a relatively deep neural network containing 18 layers. This depth enables the network to learn more features and patterns to better distinguish images of playing cards.\n",
    "- **Residual connections**\n",
    "  - ResNet-18 introduced residual connections, allowing information to pass more freely between layers in the network. This helps prevent vanishing gradient problems and makes the network easier to train.\n",
    "- **Data augmentation**\n",
    "  - When training the data, we used the data augmentation technique of random flipping. This increases the diversity of the training data, helping the network generalize better to new images.\n",
    "- **Optimization algorithm**\n",
    "  - We use the efficient Adam optimization algorithm. Adam can automatically adjust the learning rate of each parameter, which helps speed up the convergence process and reduce training time and resource costs.\n",
    "\n",
    "**Due to the above factors, the model achieved an accuracy of 92% in the test set.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
